## 16S扩增子分析

#### 背景

```properties
16S 选择测序区域 V3+V4
ITS测序主要是针对真菌多样性的研究，注释准确度高；
18S测序针对的是真核微生物，注释到的范围广，但是就真菌来说精确度相对较低
```

#### 流程

```properties
1.原始数据到OTU表格
# 双端测序数据合并
# Barcode  引物去除
# 去除嵌合体序列 
# 聚类生成OUT
# 物种注释
# OTU 过滤
# 数据标准化
OTU是人为给某一个分类单元（品系，种，属，分组等）设置的同一标志。
要了解一个样品测序结果中的菌种、菌属等数目信息，
就需要对序列进行聚类操作（cluster）。通过聚类操作，将
序列按照彼此的相似性分归为许多小组，一个小组就是一个OTU。
通常在97%的相似水平下聚类生成OTU，选择每个聚类群中最高丰度序列作
为代表性序列。发现100%更合理，即不聚类的ASV更容易实现跨研究比较。
#############################################################################
2.物种多样性分析
# 物种丰度
# alpha多样性
# beta 多样性
#############################################################################
3.OTU物种丰度差异分析
#############################################################################
4.PICRUSt细菌功能预测
#############################################################################
```

#### 文章地址：https://www.nature.com/articles/s41559-019-1063-3

#### 1. 数据合并

```shell
####docker run -it -m 8G --cpus 8 --rm -v E:/qime16:/work Kjd/amplseq
dbdir=/work/database
workdir=/work/my_16s
datadir=$workdir/data
scriptdir=$workdir/scripts
fastmap=$workdir/fastmap.txt
mkdir /work/tmp
export TMPDIR=/work/tmp  #防止临时目录存储 不够

export PATH=$workdir/scripts:$PATH   #添加代码到环境PATH中
######################database
#各大数据库地址：
silva_16S_97_seq=$dbdir/SILVA_132_QIIME_release/rep_set/rep_set_16S_only/97/silva_132_97_16S.fna 
silva_16S_97_tax=$dbdir/SILVA_132_QIIME_release/taxonomy/16S_only/97/taxonomy_7_levels.txt 

greengene_16S_97_seq=$dbdir/gg_13_8_otus/rep_set/97_otus.fasta
greengene_16S_97_tax=$dbdir/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt

silva_18S_97_seq=$dbdir/SILVA_132_QIIME_release/rep_set/rep_set_18S_only/97/silva_132_97_18S.fna 
silva_18S_97_tax=$dbdir/SILVA_132_QIIME_release/taxonomy/18S_only/97/taxonomy_7_levels.txt 

unite_ITS_97_seq=$dbdir/unite_ITS8.2/sh_refs_qiime_ver8_97_04.02.2020.fasta
unite_ITS_97_tax=$dbdir/unite_ITS8.2/sh_taxonomy_qiime_ver8_97_04.02.2020.txt

#选择使用的数据库
SEQ=$silva_16S_97_seq
TAX=$silva_16S_97_tax
```

```shell
cd $workdir  #回到工作目录
mkdir 1.merge_pe

for i in  `cat $fastmap |grep -v '#'|cut -f 1` ;do 

    flash $datadir/${i}_1.fastq.gz $datadir/${i}_2.fastq.gz \
        -m 10 -x 0.2 -p 33 -t 1 \
        -o $i -d 1.merge_pe
done
```

#### 2.对原始数据进行fastqc质控

```shell
cd $workdir  #回到工作目录
mkdir 2.fastqc
#fastqc查看数据质量分布等
fastqc -t 2 $workdir/1.merge_pe/*extendedFrags.fastq  -o $workdir/2.fastqc
#质控结果汇总
cd $workdir/2.fastqc
multiqc .
```

#### 3.数据质控：对原始序列进行去接头，删除低质量的reads

```shell
cd $workdir  #回到工作目录
mkdir 3.data_qc
cd  3.data_qc
#利用fastp工具去除adapter
#--qualified_quality_phred the quality value that a base is qualified. 
#			Default 15 means phred quality >=Q15 is qualified. (int [=15])
#--unqualified_percent_limit how many percents of bases are allowed to be unqualified
#--n_base_limit if one read's number of N base is >n_base_limit, 
#			then this read/pair is discarded 
#--detect_adapter_for_pe   接头序列未知  可设置软件自动识别常见接头
#
for i in `cat $fastmap |grep -v '#'|cut -f 1`; do 
    fastp --thread 1 --qualified_quality_phred 10 \
        --unqualified_percent_limit 50 \
        --n_base_limit 10 \
        --length_required 300 \
        --trim_front1 29 \
        --trim_tail1 18 \
        -i $workdir/1.merge_pe/${i}.extendedFrags.fastq \
        -o ${i}.clean_tags.fq.gz \
        --detect_adapter_for_pe -h ${i}.html -j ${i}.json
done

#质控数据统计汇总：
qc_stat.py -d $workdir/3.data_qc/ -o $workdir/3.data_qc/ -p all_sample_qc
```

```python
## qc_stat.py
import argparse, os, json, glob

parser = argparse.ArgumentParser(description='This script was used to stat qc result')
parser.add_argument("-d", '--qcdir', dest='qcdir', required=True, help='set fastp qc dir')
parser.add_argument("-o", '--outdir', dest='outdir', required=False, default=os.getcwd(),
                    help='specify the output file dir,default is current dir')
parser.add_argument("-p", '--prefix', dest='prefix', required=False, default='all_sample_qc',
                    help='file name prefix,default all_sample_qc')
args = parser.parse_args()
if not os.path.exists(args.outdir): os.mkdir(args.outdir)
name = os.path.abspath(args.outdir) + '/' + args.prefix
args.qcdir = os.path.abspath(args.qcdir)

files = glob.glob(args.qcdir + "/*json")

assert len(files) > 0, "failed to find fastp json files in dir: %s" % args.qcdir

outfile = open(name + ".tsv", "w")
outfile.write("SampleID\trawDataReads\tcleanDataReads\trawDataBase\tcleanDataBase\tQ20\tQ30\tGC\n")
for i in sorted(files):
    (filepath, tempfilename) = os.path.split(i)
    (samplename, extension) = os.path.splitext(tempfilename)
    f = open(i, "r")
    qcjs = json.load(f)
    rawDataReads = format(qcjs["summary"]["before_filtering"]["total_reads"], ",")  # 一对reads 记为两条
    rawDataBase = format(qcjs["summary"]["before_filtering"]["total_bases"], ",")
    cleanDataReads = format(qcjs["summary"]["after_filtering"]["total_reads"], ",")
    cleanDataBase = format(qcjs["summary"]["after_filtering"]["total_bases"], ",")

    Q20 = format(qcjs["summary"]["after_filtering"]["q20_rate"] * 100, '0.2f')
    Q30 = format(qcjs["summary"]["after_filtering"]["q30_rate"] * 100, '0.2f')
    GC = format(qcjs["summary"]["after_filtering"]["gc_content"] * 100, '0.2f')
    outfile.write(
        "\t".join([samplename, rawDataReads, cleanDataReads, rawDataBase, cleanDataBase, Q20, Q30, GC]) + "\n")
    f.close()
outfile.close()
```

#### 4.去除嵌合体

```shell
cd $workdir  #回到工作目录
mkdir 4.remove_chimeras
cd  4.remove_chimeras

#去除嵌合体

for i in `cat $fastmap |grep -v '#'|cut -f 1`; do 
     #相同重复序列合并
    vsearch --derep_fulllength $workdir/3.data_qc/${i}.clean_tags.fq.gz       \
        --sizeout --output ${i}.derep.fa
    #去嵌合体
    vsearch --uchime3_deno ${i}.derep.fa \
        --sizein --sizeout  \
        --nonchimeras ${i}.denovo.nonchimeras.rep.fa 
    #相同序列还原为多个
    vsearch --rereplicate ${i}.denovo.nonchimeras.rep.fa --output ${i}.denovo.nonchimeras.fa

done

#根据参考序列去除嵌合体(真菌没有参考序列这步不用)
for i in `cat $fastmap |grep -v '#'|cut -f 1`; do 
vsearch --uchime_ref ${i}.denovo.nonchimeras.fa \
        --db $dbdir/rdp_gold.fa \
         --sizein --sizeout --fasta_width 0 \
        --nonchimeras ${i}.ref.nonchimeras.fa
done
```

#### 5.qiime1分析pic otu 聚类方法

```shell
cd $workdir  #回到工作目录
mkdir 5.pick_otu_qiime
cd   5.pick_otu_qiime

#合并fasta文件，并加序列号
for i in `cat $fastmap |grep -v '#'|cut -f 1`; do 
    format_fa_for_qiime.pl  -f $workdir/4.remove_chimeras/$i.ref.nonchimeras.fa \
        -n $i -out $i.fa
done

#合并fa文件到qiime.fasta 之后删除所有单个样本的fa文件
cat *fa >qiime.fasta
rm -f *fa


```

```perl
### format_fa_for_qiime.pl
use strict;
use Cwd qw(abs_path getcwd);
use Getopt::Long;
use Bio::SeqIO;
use Bio::Seq;

my ($od,$fasta,$name,$out);
GetOptions(
                "help|?" =>\&USAGE,
				'f=s'=>\$fasta,
				'n=s'=>\$name,
				"od=s"=>\$od,
				"out=s"=>\$out
                ) or &USAGE;
&USAGE unless ($fasta and $name);

sub USAGE {
	my $usage=<<"USAGE";
Program: $0
Contact: huangls 
Discription:

Usage:
  Options:
  -f	<file>	Input  fa file, required
  -n	<str>	sample name ,required
  -out <str>   output file name optional
  -od	<dir>	output dir,optional
   -h		Help
Example:
perl $0 -fasta  BC2.fasta -name BC1  -out qiime.fa

USAGE
	print $usage;
	exit 1;
}

$od||=getcwd;
mkdir $od unless(-d $od);
$out||="$name.fa";

mkdir "$od/" unless(-d "$od/");

my$o = Bio::SeqIO->new(-file => ">$od/$out" ,
	                               -format => 'Fasta');

my$in="";
my  $FI;
if ($fasta=~/gz$/){
	open  $FI, "<:gzip", "$fasta" or die "$!";
	$in= Bio::SeqIO->new(-file => $FI ,
                               -alphabet=>"dna",
                               -format => 'Fasta');
	
}else{
	  $in= Bio::SeqIO->new(-file => "$fasta" ,
                               -alphabet=>"dna",
                               -format => 'Fasta');
}

my$i=0;
while ( my $seq = $in->next_seq() ) {
	$i++;
	my $id = $seq->id;
	my $new_seq=Bio::Seq->new(-id=>"${name}_$i",-seq=>$seq->seq);
	$o->write_seq($new_seq);
}
$in->close();
$o->close();
```